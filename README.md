ðŸŽ­ Multimodal Emotion and Sentiment Detection System
This project is a Multimodal Deep Learning System for Emotion and Sentiment Classification using Audio, Video, and Text modalities. It is trained and evaluated on the benchmark MELD dataset, leveraging state-of-the-art models for cross-modal affective analysis.

# Highlights
Multimodal Fusion: Combines text, audio, and video cues for more robust emotion/sentiment detection.

# Models Used:

BERT for Text

R3D-18 (3D ResNet) for Video

1D CNN for Audio

# Preprocessing:

Audio-visual segmentation using ffmpeg

Speaker-specific utterance alignment

# Performance:

Achieved 70%+ accuracy on the MELD benchmark

# Dataset: MELD (10+ GB), downloaded via Hugging Face datasets library

# Training: Google Colab with GPU acceleration (T4/A100)

![Screenshot 2025-06-30 023828](https://github.com/user-attachments/assets/4529b6d5-fab4-427e-9d79-6dddf6dcc7d0)

# Outcome 

![Screenshot 2025-06-30 101348](https://github.com/user-attachments/assets/01ac942f-b3ae-4f37-bdaf-a5859a8394d9)
